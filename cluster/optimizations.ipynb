{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simplex_proj(z):\n",
    "    \"\"\"\n",
    "    Projection sur le probability simplex\n",
    "    http://arxiv.org/pdf/1309.1541.pdf\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # for reshaping from matrix type\n",
    "    y = np.array(z).reshape(len(z)) \n",
    "    D, = y.shape\n",
    "    x = np.array(sorted(y, reverse=True))\n",
    "    u = [x[j] + 1. / (j + 1) * (1 - sum([x[i] for i in range(j + 1)])) for j in range(D)]\n",
    "    l = []\n",
    "    for idx, val in enumerate(u):\n",
    "        if val > 0 :\n",
    "            l.append(idx)\n",
    "    if l == []:\n",
    "        l.append(0)\n",
    "    rho = max(l)\n",
    "    lambd = 1. / (rho + 1) * (1 - sum([x[i] for i in range(rho + 1)]))\n",
    "    return np.array([max(yi + lambd, 0) for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "algo_root = '..'\n",
    "sys.path.insert(0, algo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tools.gm_tools import gm_params_generator, gaussian_mixture_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_different_lambdas(X, means, covars, pi, lambd, EPSILON=1e-8):\n",
    "    \"\"\"\n",
    "    Evaluate the gradient of -sum_i^n( log( sum_j^K (pi_j * phi(mu_j, sigma_j)(X_i) ))) + sum_l^K (lambda_l*pi_l)\n",
    "    \"\"\"\n",
    "    densities = np.array([multivariate_normal.pdf(X, means[i], covars[i]) for i in range(len(pi))]).T\n",
    "    #We reshape for the division and add EPSILON to avoid zero division\n",
    "    #we add the lambda penality (SLOPE like)\n",
    "    return -(densities/(((densities*pi).sum(axis=1)).reshape(X.shape[0],1) + EPSILON)).sum(axis=0) + lambd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pi_differentLambdas_estim_fista(X, means, covars, pi, L, lambd):\n",
    "    \"\"\"\n",
    "    We use FISTA to accelerate the convergence of the algorithm\n",
    "    we project the next step of the gradient descent on the probability simplex\n",
    "    \"\"\"\n",
    "    t_previous = 1\n",
    "    pi_previous = np.copy(pi)\n",
    "    xi = np.copy(pi_previous)\n",
    "    # the number of iterations is given on FISTA paper, \n",
    "    # we took ||pi_hat-pi_star||**2 = len(pi)**2\n",
    "    fista_iter = int(np.sqrt(2*len(pi)**2 * L) // 1)\n",
    "    for _ in range(min(500, fista_iter)):\n",
    "        pi_next = simplex_proj(xi - 1./(np.sqrt(X.shape[0])*L)*gradient_different_lambdas(X, means, covars, xi, lambd))\n",
    "        t_next = (1. + np.sqrt(1 + 4 * t_previous**2)) / 2\n",
    "        xi = pi_next + (t_previous - 1) / t_next * (pi_next - pi_previous)\n",
    "        pi_previous = np.copy(pi_next)\n",
    "    return pi_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.utils import check_array\n",
    "from tools.matrix_tools import check_zero_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def algo_different_lambdas_penalities_1(X, max_clusters, n_iter, L, alpha=0.01):\n",
    "    \"\"\"\n",
    "    we inject in the gradient the penality, and project the estimate in the\n",
    "    probability simplex\n",
    "    \"\"\"\n",
    "    lambd = lambda_list_BH(max_clusters, alpha)\n",
    "    # initialization of the algorithm\n",
    "    g = GMM(n_components=max_clusters, covariance_type= \"full\")\n",
    "    g.fit(X)\n",
    "    means_estim, covars_estim, pi_estim = g.means_, g.covars_, g.weights_\n",
    "    N = len(X)\n",
    "    K = len(pi_estim)\n",
    "    print \"Init EM pi: \",pi_estim\n",
    "    for it in range(n_iter):\n",
    "        # We estimate pi according to the penalities lambdas given\n",
    "        pi_estim = pi_differentLambdas_estim_fista(X, means_estim, covars_estim, pi_estim, L, lambd)\n",
    "        # we remove the clusters with probability = 0\n",
    "        non_zero_elements = np.nonzero(pi_estim)[0]\n",
    "        K = len(non_zero_elements)\n",
    "        pi_estim = np.array([pi_estim[i] for i in non_zero_elements])\n",
    "        means_estim = np.array([means_estim[i] for i in non_zero_elements]) \n",
    "        covars_estim = np.array([covars_estim[i] for i in non_zero_elements])\n",
    "        lambd = np.array([lambd[i] for i in non_zero_elements])\n",
    "        # we estimate the conditional probability P(z=j/X[i])\n",
    "        tau = tau_estim(X, means_estim, covars_estim, pi_estim)\n",
    "        # Means\n",
    "        means_estim = np.array([(tau[:, k]*X.T).sum(axis=1)*1/(N*pi_estim[k]) for k in range(K)])\n",
    "        # covars \n",
    "        covars_temp = np.array(\n",
    "                [covar_estim(X, means_estim[k], tau[:, k], pi_estim[k]) for k in range(K)])\n",
    "        non_empty_covar_idx = check_zero_matrix(covars_temp)\n",
    "        pi_estim = [pi_estim[j] for j in non_empty_covar_idx]\n",
    "        means_estim = [means_estim[j] for j in non_empty_covar_idx]\n",
    "        covars_estim = [covars_estim[j] for j in non_empty_covar_idx]\n",
    "        lambd = [lambd[j] for j in non_empty_covar_idx]\n",
    "        K = len(pi_estim)\n",
    "        if it%10 == 0 :\n",
    "            print \"iteration \",it, \"pi: \", pi_estim\n",
    "    return pi_estim, means_estim, covar_estim, tau_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tau_estim(X, centers, covars, pi):\n",
    "    try:\n",
    "        densities = np.array([multivariate_normal.pdf(X, centers[k], covars[k], allow_singular=True) for k in range(len(pi))]).T * pi\n",
    "        return (densities.T/(densities.sum(axis=1))).T\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        print \"Error on density computation for tau\", e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def covar_estim(X, mean, tau, pi):\n",
    "    \"\"\"\n",
    "    emp covariance of EM\n",
    "    :param mean: mean for one cluster\n",
    "    :param pi: pi for this cluster\n",
    "    :param N: lenth of X\n",
    "    :param tau: vector of proba for each X[i] in the cluster, given by tau[:,k]\n",
    "    :return: emp covariance matrix of this cluster\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    Z = np.sqrt(tau).reshape(N, 1) * (X - mean)\n",
    "    return 1 / (pi * N) * Z.T.dot(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_zero_matrix(mat_list):\n",
    "    \"\"\"\n",
    "    Return the list of matrices ids which are non empty\n",
    "    :param mat_list: List of matrices, usually covariance matrices\n",
    "    :return: list of ids of non empty matrices\n",
    "    \"\"\"\n",
    "    non_zero_list = []\n",
    "    for i in range(len(mat_list)):\n",
    "        if np.count_nonzero(mat_list[i]) is not 0:\n",
    "            non_zero_list.append(i)\n",
    "    return non_zero_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def main():\n",
    "#    pi, means, covars = gm_params_generator(3,3)\n",
    "#    X,_ = gaussian_mixture_sample(pi, means, covars, 1e5)\n",
    "#    pi_e = algo_different_lambdas_penalities_1(X,max_clusters=5,n_iter=500, L=1e5)\n",
    "#    return pi_e, pi\n",
    "##pi_e, pi = main()\n",
    "#print \"real pi: \", pi\n",
    "#print \"estimated pi: \", pi_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lambda_list_BH(K, alpha=1):\n",
    "    #Cf pierre bellec, Candes (Mimimax SLOPE), lambda_BH, we add a normalization\n",
    "    return alpha*np.array([np.sqrt(2./K*np.log(1.*K/(j+1))) for j in range(K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alg. optim sur le cone + proj simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_gradient(X, means, covars, pi, EPSILON=1e-8):\n",
    "    \"\"\"\n",
    "    Evaluate the gradient of -sum_i^n( log( sum_j^K (pi_j * phi(mu_j, sigma_j)(X_i) ))) \n",
    "    \"\"\"\n",
    "    densities = np.array([multivariate_normal.pdf(X, means[i], covars[i]) for i in range(len(pi))]).T\n",
    "    #We reshape for the division and add EPSILON to avoid zero division\n",
    "    #we add the lambda penality (SLOPE like)\n",
    "    return -(densities/(((densities*pi).sum(axis=1)).reshape(X.shape[0],1) + EPSILON)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cvxpy import *\n",
    "\n",
    "def ordered_optim_proj(y, lambd):\n",
    "    \"\"\"\n",
    "    We solve the optimization problem:\n",
    "    1/2*||b-x||**2 + sum_j(lambda_i*x_j) with csts: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "    \"\"\"\n",
    "    # Construct the problem.\n",
    "    n = y.shape[0]\n",
    "    x = Variable(n)    \n",
    "    objective = Minimize(1./n*sum_squares(x - y) + sum_entries(np.diag(lambd)*x))\n",
    "    #We reformulate the constrains as: x_i - x_j >= 0 i,j in [k-1] and x_k > 0\n",
    "    constraints = [(x[:n-1]-x[1:])>=0, x[-1]>0, sum_entries(x)==1]\n",
    "    prob = Problem(objective, constraints)\n",
    "    # The optimal objective is returned by prob.solve().\n",
    "    result = prob.solve(solver=CVXOPT)\n",
    "    #We project on the probability simplex\n",
    "    # The optimal value for x is stored in x.value.\n",
    "    return np.array(x.value).reshape(len(x.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pi_differentLambdas_estim_fista_conic(X, means, covars, pi, L, lambd):\n",
    "    \"\"\"\n",
    "    We use FISTA to accelerate the convergence of the algorithm\n",
    "    we project the next step of the gradient descent on the probability simplex\n",
    "    \"\"\"\n",
    "    t_previous = 1\n",
    "    pi_previous = np.copy(pi)\n",
    "    xi = np.copy(pi_previous)\n",
    "    # the number of iterations is given on FISTA paper, \n",
    "    # we took ||pi_hat-pi_star||**2 = len(pi)**2\n",
    "    fista_iter = int(np.sqrt(2*len(pi)**2 * L) // 1)\n",
    "    for _ in range(min(500, fista_iter)):\n",
    "        pi_next = ordered_optim_proj(xi - 1./(np.sqrt(X.shape[0])*L)*simple_gradient(X, means, covars, xi), lambd)\n",
    "        t_next = (1. + np.sqrt(1 + 4 * t_previous**2)) / 2\n",
    "        xi = pi_next + (t_previous - 1) / t_next * (pi_next - pi_previous)\n",
    "        pi_previous = np.copy(pi_next)\n",
    "    return pi_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def algo_different_lambdas_penalities_conic(X, max_clusters, n_iter, L, alpha=0.001):\n",
    "    \"\"\"\n",
    "    we inject in the gradient the penality, and project the estimate in the\n",
    "    probability simplex\n",
    "    \"\"\"\n",
    "    lambd = lambda_list_BH(max_clusters, alpha)\n",
    "    # initialization of the algorithm\n",
    "    g = GMM(n_components=max_clusters, covariance_type= \"full\")\n",
    "    g.fit(X)\n",
    "    # we order for slope\n",
    "    print \n",
    "    pi_estim, means_estim, covars_estim = map(list,zip(*(sorted(zip(g.weights_, g.means_, g.covars_))[::-1])))\n",
    "    print pi_estim\n",
    "    print \"Init EM pi: \",pi_estim\n",
    "    N = len(X)\n",
    "    K = len(pi_estim)\n",
    "    for it in range(n_iter):\n",
    "        # We estimate pi according to the penalities lambdas given\n",
    "        pi_estim = pi_differentLambdas_estim_fista_conic(X, means_estim, covars_estim, pi_estim, L, lambd)\n",
    "        # we remove the clusters with probability = 0\n",
    "        non_zero_elements = np.nonzero(pi_estim)[0]\n",
    "        K = len(non_zero_elements)\n",
    "        pi_estim = np.array([pi_estim[i] for i in non_zero_elements])\n",
    "        means_estim = np.array([means_estim[i] for i in non_zero_elements]) \n",
    "        covars_estim = np.array([covars_estim[i] for i in non_zero_elements])\n",
    "        lambd = np.array([lambd[i] for i in non_zero_elements])\n",
    "        # we estimate the conditional probability P(z=j/X[i])\n",
    "        tau = tau_estim(X, means_estim, covars_estim, pi_estim)\n",
    "        # Means\n",
    "        means_estim = np.array([(tau[:, k]*X.T).sum(axis=1)*1/(N*pi_estim[k]) for k in range(K)])\n",
    "        # covars \n",
    "        covars_temp = np.array(\n",
    "                [covar_estim(X, means_estim[k], tau[:, k], pi_estim[k]) for k in range(K)])\n",
    "        non_empty_covar_idx = check_zero_matrix(covars_temp)\n",
    "        pi_estim = [pi_estim[j] for j in non_empty_covar_idx]\n",
    "        means_estim = [means_estim[j] for j in non_empty_covar_idx]\n",
    "        covars_estim = [covars_estim[j] for j in non_empty_covar_idx]\n",
    "        lambd = [lambd[j] for j in non_empty_covar_idx]\n",
    "        K = len(pi_estim)\n",
    "        if it%10 == 0 :\n",
    "            print \"iteration \",it, \"pi: \", pi_estim\n",
    "    return pi_estim, means_estim, covars_estim, tau_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def view2Ddata(X):\n",
    "    from plotly.offline import plot\n",
    "    import plotly.graph_objs as go\n",
    "    \n",
    "    # Create random data with numpy\n",
    "    import numpy as np\n",
    "    \n",
    "    N = 1000\n",
    "    random_x = np.random.randn(N)\n",
    "    random_y = np.random.randn(N)\n",
    "    \n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=X[:,0],\n",
    "        y=X[:,1],\n",
    "        mode = 'markers'\n",
    "    )\n",
    "    \n",
    "    data = [trace]\n",
    "    \n",
    "    # Plot and embed in ipython notebook!\n",
    "    plot(data, filename='Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "from scipy import random, linalg\n",
    "\n",
    "def gm_params_generator(d, k, sparse_proba=None):\n",
    "    \"\"\"\n",
    "    We generate centers in [-0.5, 0.5] and verify that they are separated enough\n",
    "    \"\"\"\n",
    "    #  we scatter the unit square on k squares, the min distance is given by c/sqrt(k)\n",
    "    min_center_dist = 0.1/np.sqrt(k)\n",
    "    centers = [np.random.rand(1, d)[0]-0.5]\n",
    "    for i in range(k-1):\n",
    "        center = np.random.rand(1, d)[0]-0.5\n",
    "        distances = np.linalg.norm(\n",
    "            np.array(centers) - np.array(center),\n",
    "            axis=1)\n",
    "        while len(distances[distances < min_center_dist]) > 0:\n",
    "            center = np.random.rand(1, d)[0]-0.5\n",
    "            distances = np.linalg.norm(\n",
    "                np.array(centers) - np.array(center),\n",
    "                axis=1)\n",
    "        centers.append(center)\n",
    "    # if sparse_proba is set :\n",
    "    #    generate covariance matrix with the possibility to set the sparsity on the precision matrix, \n",
    "    # we multiply by 1/k^2 to avoid overlapping\n",
    "    if sparse_proba==None:\n",
    "        A = [random.rand(d,d) for _ in range(k)]\n",
    "        cov = [1e-2/(k**2)*(np.diag(np.ones(d))+np.dot(a,a.transpose())) for a in A]\n",
    "    else:\n",
    "        cov = np.array([np.linalg.inv(make_sparse_spd_matrix(d, alpha=sparse_proba)) for _ in range(k)])\n",
    "    p = np.random.randint(1000, size=(1, k))[0]\n",
    "    weights = 1.0*p/p.sum()\n",
    "    return weights, centers, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi, means, covars = gm_params_generator(2,3)\n",
    "X,_ = gaussian_mixture_sample(pi, means, covars, 1e4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "view2Ddata(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.38199999999905737, 0.23354070527228951, 0.18261300185416762, 0.12478700287951697, 0.077059289994969776]\n",
      "Init EM pi:  [0.38199999999905737, 0.23354070527228951, 0.18261300185416762, 0.12478700287951697, 0.077059289994969776]\n",
      "iteration  0 pi:  [0.36079735589051803, 0.2048053366755872, 0.18039893514897853, 0.13540171676685453, 0.11859665551806181]\n",
      "iteration  10 pi:  [0.36580168042241734, 0.31752237142651113, 0.31652785954893159, 0.00010425201035876645, 4.3836591781231398e-05]\n",
      "iteration  20 pi:  [0.36576731274559665, 0.31754456544023785, 0.31658075194389135, 8.0857817339771875e-05, 2.6512052934679904e-05]\n",
      "iteration  30 pi:  [0.3657673127456067, 0.31754456544023862, 0.31658075194381474, 8.0857817377907615e-05, 2.6512052962076866e-05]\n",
      "iteration  40 pi:  [0.36576731274560176, 0.31754456544024207, 0.3165807519438264, 8.0857817370609227e-05, 2.6512052959256693e-05]\n",
      "iteration  50 pi:  [0.36576731274560276, 0.31754456544025267, 0.31658075194381025, 8.0857817373903657e-05, 2.6512052960527863e-05]\n",
      "iteration  60 pi:  [0.36576731274560148, 0.31754456544024062, 0.31658075194382301, 8.0857817373883192e-05, 2.6512052960710734e-05]\n",
      "iteration  70 pi:  [0.36576731274560481, 0.31754456544025073, 0.3165807519438229, 8.0857817364391965e-05, 2.6512052956881616e-05]\n",
      "iteration  80 pi:  [0.36576731274559587, 0.31754456544023357, 0.31658075194383012, 8.085781737815339e-05, 2.651205296199304e-05]\n",
      "iteration  90 pi:  [0.36576731274560081, 0.31754456544023107, 0.31658075194382862, 8.0857817377659333e-05, 2.6512052961869583e-05]\n",
      "real pi:  [ 0.38017872  0.3151909   0.30463038]\n",
      "estimated pi:  [0.3657673127455926, 0.31754456544024012, 0.31658075194382973, 8.0857817376170493e-05, 2.6512052961342214e-05]\n",
      "real means [array([-0.27915768, -0.06191155]), array([ 0.30557406,  0.23066465]), array([ 0.40352661, -0.16597811])]\n",
      "estimated means [array([-0.29139194, -0.06488453]), array([ 0.29943912,  0.2269561 ]), array([ 0.39193418, -0.15944259]), array([ -3.47050975e-04,   3.68324216e-05]), array([ -1.84497714e-03,  -9.75333647e-05])]\n"
     ]
    }
   ],
   "source": [
    "# methode SLOPE\n",
    "# avec pi_i ordonnés\n",
    "# avec 1/2*||b-x||**2 + sum_j(lambda_i*x_j) with csts: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "#utilisation de CVXPY\n",
    "pi_e, means_e, covars_e, _  = algo_different_lambdas_penalities_conic(X,max_clusters=5,n_iter=100, L=1e5, alpha=0.0001 )\n",
    "print \"real pi: \", pi\n",
    "print \"estimated pi: \", pi_e\n",
    "print \"real means\", means\n",
    "print \"estimated means\", means_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init EM pi:  [ 0.01287115  0.08341727  0.06008517  0.0924248   0.04685816  0.0929563\n",
      "  0.0463398   0.05763489  0.10047304  0.11010895  0.02344063  0.05388548\n",
      "  0.08449907  0.08174997  0.05325531]\n",
      "iteration  0 pi:  [0.096952114535662673, 0.035597930563478118, 0.1472514696955006, 0.12942025147337394, 0.020889574779287171, 0.13482198490705372, 0.16083820238644564, 0.0072576048076471947, 0.10372399452459607, 0.12479035702780342, 0.038456515299151464]\n",
      "iteration  10 pi:  [0.22449184045514722, 0.1673470345699245, 0.14043039057206336, 0.36721165980990855, 0.014187722990262431, 0.08633135160269384]\n",
      "iteration  20 pi:  [0.27670839799732239, 0.18217736950412516, 0.12528428447821868, 0.38210039965380976, 0.033729548366524004]\n",
      "iteration  30 pi:  [0.31060419536604794, 0.19660006980892553, 0.11078341928267831, 0.38201231554234805]\n",
      "iteration  40 pi:  [0.31060658951551984, 0.21046583176558606, 0.096912318518254131, 0.38201526020064003]\n",
      "iteration  50 pi:  [0.31060896062828708, 0.22428203140477854, 0.083090831441670163, 0.38201817652526415]\n",
      "iteration  60 pi:  [0.31061130623857397, 0.23804914324728424, 0.069318489031095359, 0.38202106148304626]\n",
      "iteration  70 pi:  [0.31061362626649747, 0.25176695083058998, 0.055595507927183124, 0.38202391497572946]\n",
      "iteration  80 pi:  [0.31061592065336763, 0.26543523442643241, 0.041922107989064768, 0.38202673693113515]\n",
      "iteration  90 pi:  [0.31061818934473118, 0.27905377663792474, 0.028298506735047982, 0.38202952728229628]\n",
      "real pi:  [ 0.38017872  0.3151909   0.30463038]\n",
      "estimated pi:  [0.31062020915532951, 0.29126775819890843, 0.016080021120665172, 0.38203201152509692]\n",
      "real means [array([-0.27915768, -0.06191155]), array([ 0.30557406,  0.23066465]), array([ 0.40352661, -0.16597811])]\n",
      "estimated means [array([ 0.30611436,  0.23201553]), array([ 0.4037363 , -0.16423951]), array([ 0.40319873, -0.16410314]), array([-0.27898646, -0.06212222])]\n"
     ]
    }
   ],
   "source": [
    "# ancienne methode avec les meme lambda_i lambda_i = alpha*sqrt(2*log(max_clusters/i))\n",
    "# sans les pi_i ordonnés\n",
    "pi_e_2, means_e_2, covars_e_2, _ = algo_different_lambdas_penalities_1(X,max_clusters=5,n_iter=100, L=1e4, alpha=1)\n",
    "print \"real pi: \", pi\n",
    "print \"estimated pi: \", pi_e_2\n",
    "print \"real means\", means\n",
    "print \"estimated means\", means_e_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20000002,  0.19999999,  0.19999999,  0.2       ,  0.19999999])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La projection nous ramene sur le \"l'angle\" du simplex à la valeur 1/K\n",
    "#En utilisant les lambda_BH_i donnés dans \"SLOPE is adaptive to unknown sparsity and asymptotically minimax\" (SU, Candes 2015) \n",
    "#conseillé par Pierre B.\n",
    "#lambda_i = alpha*sqrt(2*log(max_clusters/i))\n",
    "a = np.array([ 0.2933247 ,  0.09657283,  0.24179129 , 0.10878973 , 0.25952145])\n",
    "ordered_optim_proj(a,lambda_list_BH(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28405798,  0.17898558,  0.17898551,  0.1789855 ,  0.17898543])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#en donnant un alpha faible\n",
    "a = np.array([ 0.2933247 ,  0.09657283,  0.24179129 , 0.10878973 , 0.25952145])\n",
    "ordered_optim_proj(a,lambda_list_BH(len(a),alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80235601  0.60540589  0.45202904  0.2987598   0.        ]\n",
      "[ 0.00802356  0.00605406  0.00452029  0.0029876   0.        ]\n"
     ]
    }
   ],
   "source": [
    "#En utilisant les lambda_BH_i donnés dans \"SLOPE is adaptive to unknown sparsity and asymptotically minimax\" (SU, Candes 2015) \n",
    "#conseillé par Pierre B.\n",
    "#lambda_i = alpha*sqrt(2*log(max_clusters/i))\n",
    "\n",
    "print lambda_list_BH(5)\n",
    "print lambda_list_BH(5, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50000006  0.49999994]\n",
      "[ 0.50000008  0.49999992]\n",
      "[ 0.75005225  0.24994775]\n",
      "[ 0.5  0.5]\n",
      "[ 0.33333334  0.33333333  0.33333333]\n",
      "[ 0.41322082  0.41322076  0.17355842]\n",
      "[ 0.41630399  0.41630392  0.16739208]\n"
     ]
    }
   ],
   "source": [
    "#test en dim 2\n",
    "print ordered_optim_proj(np.array([0.5,1]),np.array([1,1])) #en dehors de la projection sur la \"face\" du simplex, on arrive a l'angle\n",
    "print ordered_optim_proj(np.array([0,1]),np.array([1,1])) # pareil\n",
    "print ordered_optim_proj(np.array([1,0.5]),np.array([1,1]))\n",
    "print ordered_optim_proj(np.array([0.5,1]),np.array([10,1]))\n",
    "# ci dessus le comportement est normal\n",
    "print ordered_optim_proj(np.array([0.5, 1, 0.5]),lambda_list_BH(3, alpha=1))\n",
    "print ordered_optim_proj(np.array([0.5,1, 0.5]),lambda_list_BH(3, alpha=0.01))\n",
    "print ordered_optim_proj(np.array([0.5,1, 0.5]),lambda_list_BH(3, alpha=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCi dessous on decompose la minimisation de :\\n\\n1/2*||b-x||**2 + sum_j(lambda_i*x_j) avec les contraintes: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\\n\\nOn va d'abord faire la minimisation avec les contraintes : x_1>=x_2>=...>=x_k>0\\nPuis faire une projection sur le simplex prob.\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ci dessous on decompose la minimisation de :\n",
    "\n",
    "1/2*||b-x||**2 + sum_j(lambda_i*x_j) avec les contraintes: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "\n",
    "On va d'abord faire la minimisation avec les contraintes : x_1>=x_2>=...>=x_k>0\n",
    "Puis faire une projection sur le simplex prob.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27326591]\n",
      " [ 0.1814368 ]\n",
      " [ 0.23049041]\n",
      " [ 0.1013161 ]\n",
      " [ 0.2595216 ]]\n"
     ]
    }
   ],
   "source": [
    "def minimization(y, lambd):\n",
    "    \"\"\"\n",
    "    We solve the optimization problem:\n",
    "    1/2*||b-x||**2 + sum_j(lambda_i*x_j) with csts: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "    \"\"\"\n",
    "    # Construct the problem.\n",
    "    n = len(y)\n",
    "    x = Variable(n)    \n",
    "    objective = Minimize(1./n*sum_squares(x - y) + sum_entries(np.diag(lambd)*x))\n",
    "    #We reformulate the constrains as: x_i - x_j >= 0 i,j in [k-1] and x_k > 0\n",
    "    constraints = [x[-1]>0] + [x[i]>=0 for i in range(n-1)] \n",
    "    prob = Problem(objective, constraints)\n",
    "    # The optimal objective is returned by prob.solve().\n",
    "    result = prob.solve(solver=CVXOPT)\n",
    "    #We project on the probability simplex\n",
    "    # The optimal value for x is stored in x.value.\n",
    "    return x.value\n",
    "\n",
    "y = np.array([ 0.2933247 ,  0.19657283,  0.24179129 , 0.10878973 , 0.25952145])\n",
    "y_temp = minimization(y, lambda_list_BH(len(y), alpha=0.01))\n",
    "print y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26405975,  0.17223063,  0.22128424,  0.09210994,  0.25031543])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On remarque\n",
    "#On projette:\n",
    "simplex_proj(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulation CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
