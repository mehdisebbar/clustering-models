{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simplex_proj(z):\n",
    "    \"\"\"\n",
    "    Projection sur le probability simplex\n",
    "    http://arxiv.org/pdf/1309.1541.pdf\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # for reshaping from matrix type\n",
    "    y = np.array(z).reshape(len(z)) \n",
    "    D, = y.shape\n",
    "    x = np.array(sorted(y, reverse=True))\n",
    "    u = [x[j] + 1. / (j + 1) * (1 - sum([x[i] for i in range(j + 1)])) for j in range(D)]\n",
    "    l = []\n",
    "    for idx, val in enumerate(u):\n",
    "        if val > 0 :\n",
    "            l.append(idx)\n",
    "    if l == []:\n",
    "        l.append(0)\n",
    "    rho = max(l)\n",
    "    lambd = 1. / (rho + 1) * (1 - sum([x[i] for i in range(rho + 1)]))\n",
    "    return np.array([max(yi + lambd, 0) for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "algo_root = '..'\n",
    "sys.path.insert(0, algo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tools.gm_tools import gm_params_generator, gaussian_mixture_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_different_lambdas(X, means, covars, pi, lambd, EPSILON=1e-8):\n",
    "    \"\"\"\n",
    "    Evaluate the gradient of -sum_i^n( log( sum_j^K (pi_j * phi(mu_j, sigma_j)(X_i) ))) + sum_l^K (lambda_l*pi_l)\n",
    "    \"\"\"\n",
    "    densities = np.array([multivariate_normal.pdf(X, means[i], covars[i]) for i in range(len(pi))]).T\n",
    "    #We reshape for the division and add EPSILON to avoid zero division\n",
    "    #we add the lambda penality (SLOPE like)\n",
    "    return -(densities/(((densities*pi).sum(axis=1)).reshape(X.shape[0],1) + EPSILON)).sum(axis=0) + lambd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pi_differentLambdas_estim_fista(X, means, covars, pi, L, lambd):\n",
    "    \"\"\"\n",
    "    We use FISTA to accelerate the convergence of the algorithm\n",
    "    we project the next step of the gradient descent on the probability simplex\n",
    "    \"\"\"\n",
    "    t_previous = 1\n",
    "    pi_previous = np.copy(pi)\n",
    "    xi = np.copy(pi_previous)\n",
    "    # the number of iterations is given on FISTA paper, \n",
    "    # we took ||pi_hat-pi_star||**2 = len(pi)**2\n",
    "    fista_iter = int(np.sqrt(2*len(pi)**2 * L) // 1)\n",
    "    for _ in range(min(500, fista_iter)):\n",
    "        pi_next = simplex_proj(xi - 1./(np.sqrt(X.shape[0])*L)*gradient_different_lambdas(X, means, covars, xi, lambd))\n",
    "        t_next = (1. + np.sqrt(1 + 4 * t_previous**2)) / 2\n",
    "        xi = pi_next + (t_previous - 1) / t_next * (pi_next - pi_previous)\n",
    "        pi_previous = np.copy(pi_next)\n",
    "    return pi_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.utils import check_array\n",
    "from tools.matrix_tools import check_zero_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def algo_different_lambdas_penalities_1(X, max_clusters, n_iter, L, alpha=0.01):\n",
    "    \"\"\"\n",
    "    we inject in the gradient the penality, and project the estimate in the\n",
    "    probability simplex\n",
    "    \"\"\"\n",
    "    lambd = lambda_list_BH(max_clusters, alpha)\n",
    "    # initialization of the algorithm\n",
    "    g = GMM(n_components=max_clusters, covariance_type= \"full\")\n",
    "    g.fit(X)\n",
    "    means_estim, covars_estim, pi_estim = g.means_, g.covars_, g.weights_\n",
    "    N = len(X)\n",
    "    K = len(pi_estim)\n",
    "    print \"Init EM pi: \",pi_estim\n",
    "    for it in range(n_iter):\n",
    "        # We estimate pi according to the penalities lambdas given\n",
    "        pi_estim = pi_differentLambdas_estim_fista(X, means_estim, covars_estim, pi_estim, L, lambd)\n",
    "        # we remove the clusters with probability = 0\n",
    "        non_zero_elements = np.nonzero(pi_estim)[0]\n",
    "        K = len(non_zero_elements)\n",
    "        pi_estim = np.array([pi_estim[i] for i in non_zero_elements])\n",
    "        means_estim = np.array([means_estim[i] for i in non_zero_elements]) \n",
    "        covars_estim = np.array([covars_estim[i] for i in non_zero_elements])\n",
    "        lambd = np.array([lambd[i] for i in non_zero_elements])\n",
    "        # we estimate the conditional probability P(z=j/X[i])\n",
    "        tau = tau_estim(X, means_estim, covars_estim, pi_estim)\n",
    "        # Means\n",
    "        means_estim = np.array([(tau[:, k]*X.T).sum(axis=1)*1/(N*pi_estim[k]) for k in range(K)])\n",
    "        # covars \n",
    "        covars_temp = np.array(\n",
    "                [covar_estim(X, means_estim[k], tau[:, k], pi_estim[k]) for k in range(K)])\n",
    "        non_empty_covar_idx = check_zero_matrix(covars_temp)\n",
    "        pi_estim = [pi_estim[j] for j in non_empty_covar_idx]\n",
    "        means_estim = [means_estim[j] for j in non_empty_covar_idx]\n",
    "        covars_estim = [covars_estim[j] for j in non_empty_covar_idx]\n",
    "        lambd = [lambd[j] for j in non_empty_covar_idx]\n",
    "        K = len(pi_estim)\n",
    "        if it%10 == 0 :\n",
    "            print \"iteration \",it, \"pi: \", pi_estim\n",
    "    return pi_estim, means_estim, covar_estim, tau_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tau_estim(X, centers, covars, pi):\n",
    "    try:\n",
    "        densities = np.array([multivariate_normal.pdf(X, centers[k], covars[k], allow_singular=True) for k in range(len(pi))]).T * pi\n",
    "        return (densities.T/(densities.sum(axis=1))).T\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        print \"Error on density computation for tau\", e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def covar_estim(X, mean, tau, pi):\n",
    "    \"\"\"\n",
    "    emp covariance of EM\n",
    "    :param mean: mean for one cluster\n",
    "    :param pi: pi for this cluster\n",
    "    :param N: lenth of X\n",
    "    :param tau: vector of proba for each X[i] in the cluster, given by tau[:,k]\n",
    "    :return: emp covariance matrix of this cluster\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    Z = np.sqrt(tau).reshape(N, 1) * (X - mean)\n",
    "    return 1 / (pi * N) * Z.T.dot(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_zero_matrix(mat_list):\n",
    "    \"\"\"\n",
    "    Return the list of matrices ids which are non empty\n",
    "    :param mat_list: List of matrices, usually covariance matrices\n",
    "    :return: list of ids of non empty matrices\n",
    "    \"\"\"\n",
    "    non_zero_list = []\n",
    "    for i in range(len(mat_list)):\n",
    "        if np.count_nonzero(mat_list[i]) is not 0:\n",
    "            non_zero_list.append(i)\n",
    "    return non_zero_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def main():\n",
    "#    pi, means, covars = gm_params_generator(3,3)\n",
    "#    X,_ = gaussian_mixture_sample(pi, means, covars, 1e5)\n",
    "#    pi_e = algo_different_lambdas_penalities_1(X,max_clusters=5,n_iter=500, L=1e5)\n",
    "#    return pi_e, pi\n",
    "##pi_e, pi = main()\n",
    "#print \"real pi: \", pi\n",
    "#print \"estimated pi: \", pi_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lambda_list_BH(K, alpha=1):\n",
    "    #Cf pierre bellec, Candes (Mimimax SLOPE), lambda_BH, we add a normalization\n",
    "    return alpha*np.array([np.sqrt(2./K*np.log(1.*K/(j+1))) for j in range(K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alg. optim sur le cone + proj simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_gradient(X, means, covars, pi, EPSILON=1e-8):\n",
    "    \"\"\"\n",
    "    Evaluate the gradient of -1/n*sum_i^n( log( sum_j^K (pi_j * phi(mu_j, sigma_j)(X_i) ))) \n",
    "    \"\"\"\n",
    "    densities = np.array([multivariate_normal.pdf(X, means[i], covars[i]) for i in range(len(pi))]).T\n",
    "    #We reshape for the division and add EPSILON to avoid zero division\n",
    "    #we add the lambda penality (SLOPE like)\n",
    "    return -1./X.shape[0]*(densities/(((densities*pi).sum(axis=1)).reshape(X.shape[0],1) + EPSILON)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cvxpy import *\n",
    "\n",
    "def ordered_optim_proj(y, lambd):\n",
    "    \"\"\"\n",
    "    We solve the optimization problem:\n",
    "    1/2*||b-x||**2 + sum_j(lambda_i*x_j) with csts: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "    \"\"\"\n",
    "    # Construct the problem.\n",
    "    n = y.shape[0]\n",
    "    x = Variable(n)    \n",
    "    objective = Minimize(1./n*sum_squares(x - y) + sum_entries(np.diag(lambd)*x))\n",
    "    #We reformulate the constrains as: x_i - x_j >= 0 i,j in [k-1] and x_k > 0\n",
    "    constraints = [(x[:n-1]-x[1:])>=0, x[-1]>0, sum_entries(x)==1]\n",
    "    prob = Problem(objective, constraints)\n",
    "    # The optimal objective is returned by prob.solve().\n",
    "    result = prob.solve(solver=CVXOPT)\n",
    "    #We project on the probability simplex\n",
    "    # The optimal value for x is stored in x.value.\n",
    "    return np.array(x.value).reshape(len(x.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pi_differentLambdas_estim_fista_conic(X, means, covars, pi, L, lambd):\n",
    "    \"\"\"\n",
    "    We use FISTA to accelerate the convergence of the algorithm\n",
    "    we project the next step of the gradient descent on the probability simplex\n",
    "    \"\"\"\n",
    "    t_previous = 1\n",
    "    pi_previous = np.copy(pi)\n",
    "    xi = np.copy(pi_previous)\n",
    "    # the number of iterations is given on FISTA paper, \n",
    "    # we took ||pi_hat-pi_star||**2 = len(pi)**2\n",
    "    fista_iter = int(np.sqrt(2*len(pi)**2 * L) // 1)\n",
    "    for _ in range(min(500, fista_iter)):\n",
    "        pi_next = ordered_optim_proj(xi - 1./(np.sqrt(X.shape[0])*L)*simple_gradient(X, means, covars, xi), lambd)\n",
    "        t_next = (1. + np.sqrt(1 + 4 * t_previous**2)) / 2\n",
    "        xi = pi_next + (t_previous - 1) / t_next * (pi_next - pi_previous)\n",
    "        pi_previous = np.copy(pi_next)\n",
    "    return pi_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def algo_different_lambdas_penalities_conic(X, max_clusters, n_iter, L, alpha=0.001):\n",
    "    \"\"\"\n",
    "    we inject in the gradient the penality, and project the estimate in the\n",
    "    probability simplex\n",
    "    \"\"\"\n",
    "    lambd = lambda_list_BH(max_clusters, alpha)\n",
    "    # initialization of the algorithm\n",
    "    g = GMM(n_components=max_clusters, covariance_type= \"full\")\n",
    "    g.fit(X)\n",
    "    # we order for slope\n",
    "    print \n",
    "    pi_estim, means_estim, covars_estim = map(list,zip(*(sorted(zip(g.weights_, g.means_, g.covars_))[::-1])))\n",
    "    print pi_estim\n",
    "    print \"Init EM pi: \",pi_estim\n",
    "    N = len(X)\n",
    "    K = len(pi_estim)\n",
    "    for it in range(n_iter):\n",
    "        # We estimate pi according to the penalities lambdas given\n",
    "        pi_estim = pi_differentLambdas_estim_fista_conic(X, means_estim, covars_estim, pi_estim, L, lambd)\n",
    "        # we remove the clusters with probability = 0\n",
    "        non_zero_elements = np.nonzero(pi_estim)[0]\n",
    "        K = len(non_zero_elements)\n",
    "        pi_estim = np.array([pi_estim[i] for i in non_zero_elements])\n",
    "        means_estim = np.array([means_estim[i] for i in non_zero_elements]) \n",
    "        covars_estim = np.array([covars_estim[i] for i in non_zero_elements])\n",
    "        lambd = np.array([lambd[i] for i in non_zero_elements])\n",
    "        # we estimate the conditional probability P(z=j/X[i])\n",
    "        tau = tau_estim(X, means_estim, covars_estim, pi_estim)\n",
    "        # Means\n",
    "        means_estim = np.array([(tau[:, k]*X.T).sum(axis=1)*1/(N*pi_estim[k]) for k in range(K)])\n",
    "        # covars \n",
    "        covars_temp = np.array(\n",
    "                [covar_estim(X, means_estim[k], tau[:, k], pi_estim[k]) for k in range(K)])\n",
    "        non_empty_covar_idx = check_zero_matrix(covars_temp)\n",
    "        pi_estim = [pi_estim[j] for j in non_empty_covar_idx]\n",
    "        means_estim = [means_estim[j] for j in non_empty_covar_idx]\n",
    "        covars_estim = [covars_estim[j] for j in non_empty_covar_idx]\n",
    "        lambd = [lambd[j] for j in non_empty_covar_idx]\n",
    "        K = len(pi_estim)\n",
    "        if it%10 == 0 :\n",
    "            print \"iteration \",it, \"pi: \", pi_estim\n",
    "    return pi_estim, means_estim, covars_estim, tau_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def view2Ddata(X):\n",
    "    from plotly.offline import plot\n",
    "    import plotly.graph_objs as go\n",
    "    \n",
    "    # Create random data with numpy\n",
    "    import numpy as np\n",
    "    \n",
    "    N = 1000\n",
    "    random_x = np.random.randn(N)\n",
    "    random_y = np.random.randn(N)\n",
    "    \n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=X[:,0],\n",
    "        y=X[:,1],\n",
    "        mode = 'markers'\n",
    "    )\n",
    "    \n",
    "    data = [trace]\n",
    "    \n",
    "    # Plot and embed in ipython notebook!\n",
    "    plot(data, filename='Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "\n",
    "def gm_params_generator(d, k, sparse_proba=None):\n",
    "    \"\"\"\n",
    "    We generate centers in [-0.5, 0.5] and verify that they are separated enough\n",
    "    \"\"\"\n",
    "    #  we scatter the unit square on k squares, the min distance is given by c/sqrt(k)\n",
    "    min_center_dist = 0.1/np.sqrt(k)\n",
    "    centers = [np.random.rand(1, d)[0]-0.5]\n",
    "    for i in range(k-1):\n",
    "        center = np.random.rand(1, d)[0]-0.5\n",
    "        distances = np.linalg.norm(\n",
    "            np.array(centers) - np.array(center),\n",
    "            axis=1)\n",
    "        while len(distances[distances < min_center_dist]) > 0:\n",
    "            center = np.random.rand(1, d)[0]-0.5\n",
    "            distances = np.linalg.norm(\n",
    "                np.array(centers) - np.array(center),\n",
    "                axis=1)\n",
    "        centers.append(center)\n",
    "    # if sparse_proba is set :\n",
    "    #    generate covariance matrix with the possibility to set the sparsity on the precision matrix, \n",
    "    # we multiply by 1/k^2 to avoid overlapping\n",
    "    if sparse_proba==None:\n",
    "        A = [random.rand(d,d) for _ in range(k)]\n",
    "        cov = [1e-2/(k**2)*(np.diag(np.ones(d))+np.dot(a,a.transpose())) for a in A]\n",
    "    else:\n",
    "        cov = np.array([np.linalg.inv(make_sparse_spd_matrix(d, alpha=sparse_proba)) for _ in range(k)])\n",
    "    p = np.random.randint(1000, size=(1, k))[0]\n",
    "    weights = 1.0*p/p.sum()\n",
    "    return weights, centers, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi, means, covars = gm_params_generator(2,3)\n",
    "X,_ = gaussian_mixture_sample(pi, means, covars, 1e4)\n",
    "view2Ddata(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.30169131291594298, 0.27996295018082018, 0.22744297633833038, 0.13650407707339524, 0.05439868349151232]\n",
      "Init EM pi:  [0.30169131291594298, 0.27996295018082018, 0.22744297633833038, 0.13650407707339524, 0.05439868349151232]\n",
      "iteration  0 pi:  [0.26467211524795647, 0.26417334677167775, 0.2368065377087514, 0.13320742999211746, 0.10114057027949686]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-98d85485ec9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# avec 1/2*||b-x||**2 + sum_j(lambda_i*x_j) with csts: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#utilisation de CVXPY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpi_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovars_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0malgo_different_lambdas_penalities_conic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"real pi: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"estimated pi: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-cc8fc7f8ff30>\u001b[0m in \u001b[0;36malgo_different_lambdas_penalities_conic\u001b[0;34m(X, max_clusters, n_iter, L, alpha)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# We estimate pi according to the penalities lambdas given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpi_estim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi_differentLambdas_estim_fista_conic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_estim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovars_estim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_estim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# we remove the clusters with probability = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnon_zero_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi_estim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-7adc0e0bb684>\u001b[0m in \u001b[0;36mpi_differentLambdas_estim_fista_conic\u001b[0;34m(X, means, covars, pi, L, lambd)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfista_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfista_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpi_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordered_optim_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msimple_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mt_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt_previous\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi_next\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_previous\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mt_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpi_next\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpi_previous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-90f32a92377f>\u001b[0m in \u001b[0;36mordered_optim_proj\u001b[0;34m(y, lambd)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# The optimal objective is returned by prob.solve().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCVXOPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#We project on the probability simplex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# The optimal value for x is stored in x.value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/cvxpy/problems/problem.pyc\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/cvxpy/problems/problem.pyc\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, solver, ignore_dcp, warm_start, verbose, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             results_dict = solver.solve(objective, constraints,\n\u001b[1;32m    278\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                                         kwargs)\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;31m# Presolve determined problem was unbounded or infeasible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/cvxpy/problems/solvers/cvxopt_intf.pyc\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, objective, constraints, cached_data, warm_start, verbose, solver_opts)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mresults_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpl_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mresults_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconelp_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Catch exceptions in CVXOPT and convert them to solver errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/cvxpy/problems/solvers/cvxopt_intf.pyc\u001b[0m in \u001b[0;36mconelp_solve\u001b[0;34m(self, data, kktsolver)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                      \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                                      \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                                      kktsolver=kktsolver)\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/cvxopt/coneprog.pyc\u001b[0m in \u001b[0;36mconelp\u001b[0;34m(c, G, h, dims, A, b, primalstart, dualstart, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mind2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0mblas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m                 blas.copy(lmbdasq, ds, n = m, offsetx = ind2, \n\u001b[1;32m   1279\u001b[0m                     offsety = ind, incy = m+1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# methode SLOPE\n",
    "# avec pi_i ordonnés\n",
    "# avec 1/2*||b-x||**2 + sum_j(lambda_i*x_j) with csts: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "#utilisation de CVXPY\n",
    "pi_e, means_e, covars_e, _  = algo_different_lambdas_penalities_conic(X,max_clusters=5,n_iter=100, L=1e5, alpha=0.0001 )\n",
    "print \"real pi: \", pi\n",
    "print \"estimated pi: \", pi_e\n",
    "print \"real means\", means\n",
    "print \"estimated means\", means_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ancienne methode avec les meme lambda_i lambda_i = alpha*sqrt(2*log(max_clusters/i))\n",
    "# sans les pi_i ordonnés\n",
    "pi_e_2, means_e_2, covars_e_2, _ = algo_different_lambdas_penalities_1(X,max_clusters=5,n_iter=100, L=1e4, alpha=1)\n",
    "print \"real pi: \", pi\n",
    "print \"estimated pi: \", pi_e_2\n",
    "print \"real means\", means\n",
    "print \"estimated means\", means_e_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20000002,  0.19999999,  0.19999999,  0.2       ,  0.19999999])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La projection nous ramene sur le \"l'angle\" du simplex à la valeur 1/K\n",
    "#En utilisant les lambda_BH_i donnés dans \"SLOPE is adaptive to unknown sparsity and asymptotically minimax\" (SU, Candes 2015) \n",
    "#conseillé par Pierre B.\n",
    "#lambda_i = alpha*sqrt(2*log(max_clusters/i))\n",
    "a = np.array([ 0.2933247 ,  0.09657283,  0.24179129 , 0.10878973 , 0.25952145])\n",
    "ordered_optim_proj(a,lambda_list_BH(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28405798,  0.17898558,  0.17898551,  0.1789855 ,  0.17898543])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#en donnant un alpha faible\n",
    "a = np.array([ 0.2933247 ,  0.09657283,  0.24179129 , 0.10878973 , 0.25952145])\n",
    "ordered_optim_proj(a,lambda_list_BH(len(a),alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80235601  0.60540589  0.45202904  0.2987598   0.        ]\n",
      "[ 0.00802356  0.00605406  0.00452029  0.0029876   0.        ]\n"
     ]
    }
   ],
   "source": [
    "#En utilisant les lambda_BH_i donnés dans \"SLOPE is adaptive to unknown sparsity and asymptotically minimax\" (SU, Candes 2015) \n",
    "#conseillé par Pierre B.\n",
    "#lambda_i = alpha*sqrt(2*log(max_clusters/i))\n",
    "\n",
    "print lambda_list_BH(5)\n",
    "print lambda_list_BH(5, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50000006  0.49999994]\n",
      "[ 0.50000008  0.49999992]\n",
      "[ 0.75005225  0.24994775]\n",
      "[ 0.5  0.5]\n",
      "[ 0.33333334  0.33333333  0.33333333]\n",
      "[ 0.41322082  0.41322076  0.17355842]\n",
      "[ 0.41630399  0.41630392  0.16739208]\n"
     ]
    }
   ],
   "source": [
    "#test en dim 2\n",
    "print ordered_optim_proj(np.array([0.5,1]),np.array([1,1])) #en dehors de la projection sur la \"face\" du simplex, on arrive a l'angle\n",
    "print ordered_optim_proj(np.array([0,1]),np.array([1,1])) # pareil\n",
    "print ordered_optim_proj(np.array([1,0.5]),np.array([1,1]))\n",
    "print ordered_optim_proj(np.array([0.5,1]),np.array([10,1]))\n",
    "# ci dessus le comportement est normal\n",
    "print ordered_optim_proj(np.array([0.5, 1, 0.5]),lambda_list_BH(3, alpha=1))\n",
    "print ordered_optim_proj(np.array([0.5,1, 0.5]),lambda_list_BH(3, alpha=0.01))\n",
    "print ordered_optim_proj(np.array([0.5,1, 0.5]),lambda_list_BH(3, alpha=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCi dessous on decompose la minimisation de :\\n\\n1/2*||b-x||**2 + sum_j(lambda_i*x_j) avec les contraintes: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\\n\\nOn va d'abord faire la minimisation avec les contraintes : x_1>=x_2>=...>=x_k>0\\nPuis faire une projection sur le simplex prob.\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ci dessous on decompose la minimisation de :\n",
    "\n",
    "1/2*||b-x||**2 + sum_j(lambda_i*x_j) avec les contraintes: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "\n",
    "On va d'abord faire la minimisation avec les contraintes : x_1>=x_2>=...>=x_k>0\n",
    "Puis faire une projection sur le simplex prob.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27326591]\n",
      " [ 0.1814368 ]\n",
      " [ 0.23049041]\n",
      " [ 0.1013161 ]\n",
      " [ 0.2595216 ]]\n"
     ]
    }
   ],
   "source": [
    "def minimization(y, lambd):\n",
    "    \"\"\"\n",
    "    We solve the optimization problem:\n",
    "    1/2*||b-x||**2 + sum_j(lambda_i*x_j) with csts: x_1>=x_2>=...>=x_k>0 and sum(x_i) = 1\n",
    "    \"\"\"\n",
    "    # Construct the problem.\n",
    "    n = len(y)\n",
    "    x = Variable(n)    \n",
    "    objective = Minimize(1./n*sum_squares(x - y) + sum_entries(np.diag(lambd)*x))\n",
    "    #We reformulate the constrains as: x_i - x_j >= 0 i,j in [k-1] and x_k > 0\n",
    "    constraints = [x[-1]>0] + [x[i]>=0 for i in range(n-1)] \n",
    "    prob = Problem(objective, constraints)\n",
    "    # The optimal objective is returned by prob.solve().\n",
    "    result = prob.solve(solver=CVXOPT)\n",
    "    #We project on the probability simplex\n",
    "    # The optimal value for x is stored in x.value.\n",
    "    return x.value\n",
    "\n",
    "y = np.array([ 0.2933247 ,  0.19657283,  0.24179129 , 0.10878973 , 0.25952145])\n",
    "y_temp = minimization(y, lambda_list_BH(len(y), alpha=0.01))\n",
    "print y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26405975,  0.17223063,  0.22128424,  0.09210994,  0.25031543])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On remarque\n",
    "#On projette:\n",
    "simplex_proj(y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasso with square root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj_circle(v):\n",
    "    \"\"\"\n",
    "    we receive a vector [v1,v2,...,vp] and project it on the unit circle.\n",
    "    \"\"\"\n",
    "    return v/np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_sqrt_penalty(X, means, covars, alpha, lambd, EPSILON=1e-8):\n",
    "    \"\"\"\n",
    "    Evaluate the gradient of -1/n*sum_i^n( log( sum_j^K (alpha_j^2 * phi(mu_j, sigma_j)(X_i) ))) + lambd*sum_j^K (alpha_j)\n",
    "    \"\"\"\n",
    "    densities = np.array([multivariate_normal.pdf(X, means[i], covars[i]) for i in range(len(alpha))]).T\n",
    "    #We reshape for the division and add EPSILON to avoid zero division\n",
    "    #we add the lambda penality \n",
    "    return -1./X.shape[0]*(densities*2*alpha/(((densities*alpha**2).sum(axis=1)).reshape(X.shape[0],1) + EPSILON)).sum(axis=0) + lambd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pi_sqrt_lasso_estim_fista(X, means, covars, pi, L, lambd):\n",
    "    \"\"\"\n",
    "    lasso with square root of pi estimation\n",
    "    We use FISTA to accelerate the convergence of the algorithm\n",
    "    we project the next step (squared) of the gradient descent on the unit circle\n",
    "    \"\"\"\n",
    "    t_previous = 1\n",
    "    alpha_previous = np.copy(pi)\n",
    "    xi = np.copy(alpha_previous)\n",
    "    # the number of iterations is given on FISTA paper, \n",
    "    # we took ||pi_hat-pi_star||**2 = len(pi)**2\n",
    "    fista_iter = int(np.sqrt(2*len(pi)**2 * L) // 1)\n",
    "    for _ in range(max(100, fista_iter)):\n",
    "        alpha_next = proj_circle(xi - 1./(np.sqrt(X.shape[0])*L)*grad_sqrt_penalty(X, means, covars, xi, lambd))\n",
    "        t_next = (1. + np.sqrt(1 + 4 * t_previous**2)) / 2\n",
    "        xi = alpha_next + (t_previous - 1) / t_next * (alpha_next - alpha_previous)\n",
    "        alpha_previous = np.copy(alpha_next)\n",
    "    #We return the squared vector to obtain a probability vector sum = 1\n",
    "    return alpha_next**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def algo__sqrt_lasso(X, max_clusters, n_iter, L, lambd=1):\n",
    "    \"\"\"\n",
    "    we inject in the gradient the penality, and project the estimate in the\n",
    "    probability simplex\n",
    "    \"\"\"\n",
    "    # initialization of the algorithm\n",
    "    g = GMM(n_components=max_clusters, covariance_type= \"full\")\n",
    "    g.fit(X)\n",
    "    # we order for slope\n",
    "    #pi_estim, means_estim, covars_estim = map(list,zip(*(sorted(zip(g.weights_, g.means_, g.covars_))[::-1])))\n",
    "    means_estim, covars_estim, pi_estim = g.means_, g.covars_, g.weights_\n",
    "    print \"Init EM pi: \",pi_estim\n",
    "    N = len(X)\n",
    "    K = len(pi_estim)\n",
    "    for it in range(n_iter):\n",
    "        # We estimate pi according to the penalities lambdas given\n",
    "        pi_estim = pi_sqrt_lasso_estim_fista(X, means_estim, covars_estim, pi_estim, L, lambd)\n",
    "        # we remove the clusters with probability = 0\n",
    "        non_zero_elements = np.nonzero(pi_estim)[0]\n",
    "        K = len(non_zero_elements)\n",
    "        pi_estim = np.array([pi_estim[i] for i in non_zero_elements])\n",
    "        means_estim = np.array([means_estim[i] for i in non_zero_elements]) \n",
    "        covars_estim = np.array([covars_estim[i] for i in non_zero_elements])\n",
    "        # we estimate the conditional probability P(z=j/X[i])\n",
    "        tau = tau_estim(X, means_estim, covars_estim, pi_estim)\n",
    "        # Means\n",
    "        means_estim = np.array([(tau[:, k]*X.T).sum(axis=1)*1/(N*pi_estim[k]) for k in range(K)])\n",
    "        # covars \n",
    "        covars_temp = np.array(\n",
    "                [covar_estim(X, means_estim[k], tau[:, k], pi_estim[k]) for k in range(K)])\n",
    "        non_empty_covar_idx = check_zero_matrix(covars_temp)\n",
    "        pi_estim = [pi_estim[j] for j in non_empty_covar_idx]\n",
    "        means_estim = [means_estim[j] for j in non_empty_covar_idx]\n",
    "        covars_estim = [covars_estim[j] for j in non_empty_covar_idx]\n",
    "        K = len(pi_estim)\n",
    "        if it%10 == 0 :\n",
    "            print \"iteration \",it, \"pi: \", pi_estim\n",
    "    return pi_estim, means_estim, covars_estim, tau_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real pi:  [ 0.4005168   0.25374677  0.34573643]\n",
      "Init EM pi:  [ 0.19611196  0.19028569  0.15186744  0.18118628  0.05988816  0.19313256\n",
      "  0.02752792]\n",
      "iteration  0 pi:  [0.13364902793129213, 0.13843301205467257, 0.16409573000553776, 0.1406961647416419, 0.13424469820574425, 0.15705899289162592, 0.13182237416948539]\n",
      "iteration  10 pi:  [0.093859060182531082, 0.35813637731986703, 0.13317731888787929, 0.12822109065347748, 0.28660615295624525]\n",
      "iteration  20 pi:  [0.20491729427780861, 0.44751164416117389, 0.34757106156101769]\n",
      "iteration  30 pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "iteration  40 pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "iteration  50 pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "iteration  60 pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "iteration  70 pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "iteration  80 pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "iteration  90 pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "estimated pi:  [0.20498526358504812, 0.4474069631803807, 0.34760777323457126]\n",
      "real means [array([-0.2628713 ,  0.38141728]), array([-0.29261808,  0.00419388]), array([ 0.05386082, -0.14023081])]\n",
      "estimated means [array([-0.3639623 ,  0.00578967]), array([-0.23146398,  0.33852847]), array([ 0.05377633, -0.13856112])]\n"
     ]
    }
   ],
   "source": [
    "# methode (square root) lasso \n",
    "# avec pi_i non ordonnés\n",
    "# avec -1/n*sum_i^n( log( sum_j^K (alpha_j^2 * phi(mu_j, sigma_j)(X_i) ))) + lambd*sum_j^K (alpha_j)\n",
    "print \"real pi: \", pi\n",
    "pi_e, means_e, covars_e, _  = algo__sqrt_lasso(X,max_clusters=7,n_iter=100, L=1, lambd=1 )\n",
    "print \"estimated pi: \", pi_e\n",
    "print \"real means\", means\n",
    "print \"estimated means\", means_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# square root lasso en faisant sur p-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi, means, covars = gm_params_generator(2,3)\n",
    "means = [np.array([0,0]), np.array([1,1]), np.array([0,2])]\n",
    "X,_ = gaussian_mixture_sample(pi, means, covars, 1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view2Ddata(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj_unit_disk(v):\n",
    "    \"\"\"\n",
    "    we receive a vector [v1,v2,...,vp] and project [v1,v2,...,vp-1] on the unit disk.\n",
    "    \"\"\"\n",
    "    if np.linalg.norm(v)**2 <= 1:\n",
    "        return v\n",
    "    else:\n",
    "        return v/np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_sqrt_penalty(X, means, covars, alpha, lambd, EPSILON=1e-8):\n",
    "    \"\"\"\n",
    "    alpha is of dim p-1\n",
    "    density is of dim p-1\n",
    "    Evaluate the gradient of \n",
    "    \"\"\"\n",
    "    dens_last_comp = multivariate_normal.pdf(X, means[len(alpha)], covars[len(alpha)]).reshape(X.shape[0],1)\n",
    "    dens_witht_p_comp = np.array([multivariate_normal.pdf(X, means[i], covars[i]) for i in range(len(alpha))]).T - dens_last_comp \n",
    "    #We reshape for the division and add EPSILON to avoid zero division\n",
    "    #we add the lambda penality \n",
    "    return lambd - 2./X.shape[0]*(alpha*dens_witht_p_comp/(EPSILON+dens_last_comp+((alpha**2)*dens_witht_p_comp).sum(axis=1).reshape(X.shape[0],1))).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pi_sqrt_lasso_reduced_estim_fista(X, means, covars, pi, L, lambd):\n",
    "    \"\"\"\n",
    "    lasso with square root of pi estimation\n",
    "    We use FISTA to accelerate the convergence of the algorithm\n",
    "    we project the next step (squared) of the gradient descent on the unit circle\n",
    "    \"\"\"\n",
    "    t_previous = 1\n",
    "    #we delete the last element and take the square root of the vector\n",
    "    alpha_previous = np.copy(np.sqrt(pi[:-1]))\n",
    "    xi = np.copy(alpha_previous)\n",
    "    # the number of iterations is given on FISTA paper, \n",
    "    # we took ||pi_hat-pi_star||**2 = len(pi)**2\n",
    "    fista_iter = int(np.sqrt(2*len(pi)**2 * L) // 1)\n",
    "    for _ in range(500):\n",
    "        grad_step = xi - 1./(np.sqrt(X.shape[0])*L)*grad_sqrt_penalty(X, means, covars, xi, lambd)\n",
    "        alpha_next = proj_unit_disk(grad_step)\n",
    "        t_next = (1. + np.sqrt(1 + 4 * t_previous**2)) / 2\n",
    "        xi = alpha_next + (t_previous - 1) / t_next * (alpha_next - alpha_previous)\n",
    "        alpha_previous = np.copy(alpha_next)\n",
    "    #We return the squared vector to obtain a probability vector sum = 1\n",
    "    return np.append(alpha_next**2, max(0,1-np.linalg.norm(alpha_next)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def algo__sqrt_reduced_lasso(X, max_clusters, n_iter, L, lambd=1):\n",
    "    \"\"\"\n",
    "    we inject in the gradient the penality, and project the estimate in the\n",
    "    probability simplex\n",
    "    \"\"\"\n",
    "    # initialization of the algorithm\n",
    "    g = GMM(n_components=max_clusters, covariance_type= \"full\")\n",
    "    g.fit(X)\n",
    "    # we order for slope\n",
    "    #pi_estim, means_estim, covars_estim = map(list,zip(*(sorted(zip(g.weights_, g.means_, g.covars_))[::-1])))\n",
    "    means_estim, covars_estim, pi_estim = g.means_, g.covars_, g.weights_\n",
    "    print \"Init EM pi: \",pi_estim\n",
    "    N = len(X)\n",
    "    K = len(pi_estim)\n",
    "    for it in range(n_iter):\n",
    "        # We estimate pi according to the penalities lambdas given\n",
    "        pi_estim = pi_sqrt_lasso_reduced_estim_fista(X, means_estim, covars_estim, pi_estim, L, lambd)\n",
    "        # we remove the clusters with probability = 0\n",
    "        non_zero_elements = np.nonzero(pi_estim)[0]\n",
    "        K = len(non_zero_elements)\n",
    "        pi_estim = np.array([pi_estim[i] for i in non_zero_elements])\n",
    "        means_estim = np.array([means_estim[i] for i in non_zero_elements]) \n",
    "        covars_estim = np.array([covars_estim[i] for i in non_zero_elements])\n",
    "        # we estimate the conditional probability P(z=j/X[i])\n",
    "        tau = tau_estim(X, means_estim, covars_estim, pi_estim)\n",
    "        # Means\n",
    "        means_estim = np.array([(tau[:, k]*X.T).sum(axis=1)*1/(N*pi_estim[k]) for k in range(K)])\n",
    "        # covars \n",
    "        covars_temp = np.array(\n",
    "                [covar_estim(X, means_estim[k], tau[:, k], pi_estim[k]) for k in range(K)])\n",
    "        non_empty_covar_idx = check_zero_matrix(covars_temp)\n",
    "        pi_estim = [pi_estim[j] for j in non_empty_covar_idx]\n",
    "        means_estim = [means_estim[j] for j in non_empty_covar_idx]\n",
    "        covars_estim = [covars_estim[j] for j in non_empty_covar_idx]\n",
    "        K = len(pi_estim)\n",
    "        if it%10 == 0 :\n",
    "            print \"iteration \",it, \"pi: \", pi_estim\n",
    "    return pi_estim, means_estim, covars_estim, tau_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0179412257799\n",
      "real pi:  [ 0.32752613  0.56329849  0.10917538]\n",
      "Init EM pi:  [ 0.13298734  0.25140046  0.1084      0.19421266  0.31299954]\n",
      "iteration  0 pi:  [0.12079342944343699, 0.14153310053961654, 0.10642280622088783, 0.20213441501846238, 0.42911624877759624]\n",
      "iteration  10 pi:  [0.0017190703487969773, 0.008225589847682839, 0.10621187654239497, 0.31556205807308929, 0.56828140518803605]\n",
      "iteration  20 pi:  [0.00096353973676235806, 0.0063777873030011365, 0.10623370987653039, 0.31802377658496167, 0.56840118649874449]\n",
      "iteration  30 pi:  [0.00012194930200124043, 0.0090047990496317631, 0.10625726831427418, 0.31608539515516365, 0.56853058817892921]\n",
      "iteration  40 pi:  [9.4543129696924623e-06, 0.006641229693130097, 0.10626433704947924, 0.31851559478546843, 0.56856938415895253]\n",
      "iteration  50 pi:  [0.00012296537012650093, 0.0082349217723062251, 0.10626178310586859, 0.31682495151267109, 0.56855537823902758]\n",
      "iteration  60 pi:  [1.0801529443002688e-05, 0.0064205703884382924, 0.10626597661036494, 0.31872426258510683, 0.56857838888664691]\n",
      "iteration  70 pi:  [0.00013393745841918051, 0.0090795943388801113, 0.106257323126658, 0.31599825463519271, 0.56853089044085003]\n",
      "iteration  80 pi:  [2.0263137388369444e-06, 0.0068928794672386976, 0.10626119303907604, 0.31829178604302633, 0.56855211513692006]\n",
      "iteration  90 pi:  [0.00016267430523324381, 0.0089561510921956303, 0.10625909577297296, 0.31608145258465842, 0.56854062624493973]\n",
      "estimated pi:  [0.0064675132201130791, 0.00017613195040163532, 0.10627481086649546, 0.31845476357705449, 0.56862678038593528]\n",
      "real means [array([0, 0]), array([1, 1]), array([0, 2])]\n",
      "estimated means [array([ 0.00032987, -0.00019478]), array([ 0.00038642, -0.00040636]), array([ -7.01798049e-05,   2.04137792e+00]), array([ 0.00032975, -0.00020301]), array([ 0.99331131,  0.99380204])]\n"
     ]
    }
   ],
   "source": [
    "# methode (square root) lasso \n",
    "# avec pi_i non ordonnés\n",
    "max_clusters = 5\n",
    "lambd = np.sqrt(2*np.log(max_clusters)/X.shape[0])\n",
    "print lambd\n",
    "print \"real pi: \", pi\n",
    "pi_e, means_e, covars_e, _  = algo__sqrt_reduced_lasso(X,max_clusters=max_clusters, n_iter=100, L=1, lambd=lambd)\n",
    "print \"estimated pi: \", pi_e\n",
    "print \"real means\", means\n",
    "print \"estimated means\", means_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07416832,  0.22048786,  0.19421025,  0.07091502])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ 0.27233861 , 0.46956135,  0.44069292 , 0.26629875])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_grad(x):\n",
    "    xi = np.copy(x)\n",
    "    L = 1\n",
    "    lambd = 0\n",
    "    for _ in range(100):\n",
    "        xi = xi - 1./(np.sqrt(X.shape[0])*L)*grad_sqrt_penalty(X, means, covars, xi, lambd)\n",
    "    return xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63166424,  0.50596471])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_grad([0.5,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1.0, num=11)\n",
    "y = np.linspace(0, 1.0, num=11)\n",
    "e = np.array(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.]\n",
      "[ 0.46502424  0.65454385]\n",
      "[ 0.46535442  0.65431305]\n",
      "[ 0.46580468  0.6539983 ]\n",
      "[ 0.46628565  0.65366055]\n",
      "[ 0.46669771  0.65336969]\n",
      "[ 0.46696407  0.65318082]\n",
      "[ 0.46654868  0.65347478]\n",
      "[ 1.578448    1.89279789]\n",
      "[ 1.61203306  1.9144976 ]\n",
      "[ 1.65791197  1.94695758]\n"
     ]
    }
   ],
   "source": [
    "for el in e:\n",
    "    print test_grad(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation + gridsearch_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "algo_root = '..'\n",
    "sys.path.insert(0, algo_root)\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from tools.gm_tools import gm_params_generator, gaussian_mixture_sample, covar_estim, score, tau_estim\n",
    "from tools.algorithms_benchmark import view2Ddata\n",
    "from tools.gm_tools import score\n",
    "from cluster.sq_root_lasso import sqrt_lasso_gmm\n",
    "from sklearn.mixture import GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi, means, covars = gm_params_generator(2,3)\n",
    "X,_ = gaussian_mixture_sample(pi, means, covars, 1e5)\n",
    "#view2Ddata(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_test = train_test_split(\n",
    "    X, np.zeros(len(X)), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grid search on sq_root_lasso method\n",
    "max_clusters = 7\n",
    "lambd = np.sqrt(2*np.log(max_clusters)/X_train.shape[0])\n",
    "param = {\"lambda_param\":[lambd, lambd+1e-1, lambd+1, lambd+10, lambd+100], \"Lipshitz_c\":[1, 10, 100, 1000, 10000], \"max_clusters\":[max_clusters]}\n",
    "clf = GridSearchCV(estimator=sqrt_lasso_gmm(), param_grid=param, cv=5, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we define a bic scoring method for the grid search\n",
    "def bic_scorer(estimator, X, y=None):\n",
    "    return (-2*score(X, estimator.weights_, estimator.means_, estimator.covars_ ) +\n",
    "            estimator._n_parameters()*np.log(X.shape[0]))\n",
    "params_GMM={\"n_components\":range(2,8), \"covariance_type\":[\"full\"]}\n",
    "clf_gmm = GridSearchCV(GMM(), param_grid=params_GMM, cv=5, n_jobs=-1, scoring=bic_scorer)\n",
    "clf_gmm.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we evaluate the loglikelihood of the fitted models on X_validation\n",
    "print \"real loglikelihood: \", 1/X_validation.shape[0]*score(X_validation, pi, means, covars)\n",
    "print \"###sq_root lasso method###\"\n",
    "print \"sq_root lasso method loglikelihood:\", 1/X_validation.shape[0]*score(X_validation, clf.best_estimator_.pi_, clf.best_estimator_.means_, clf.best_estimator_.covars_)\n",
    "print \"pi: \", clf_gmm.best_estimator_.weights_\n",
    "print \"means: \", clf_gmm.best_estimator_.means_\n",
    "print \"###EM###\"\n",
    "print \"EM loglikelihood:\", 1/X_validation.shape[0]*score(X_validation, clf_gmm.best_estimator_.weights_, clf_gmm.best_estimator_.means_, clf_gmm.best_estimator_.covars_)\n",
    "print \"pi: \", clf_gmm.best_estimator_.weights_\n",
    "print \"means: \", clf_gmm.best_estimator_.means_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
